# Parallel-Programming-with-MPI-OpenMP-and-CUDA

## Useful Packages with Python

- [Threading](https://docs.python.org/3/library/threading.html)
- [Multiprocessing](https://docs.python.org/3/library/multiprocessing.html)
- [IPyparallel](https://ipyparallel.readthedocs.io/en/latest/)
- [Pathos](https://pathos.readthedocs.io/en/latest/index.html)
- [Numba](https://numba.readthedocs.io/en/stable/user/index.html)
- [PyTorch_Multiprocessing](https://pytorch.org/docs/stable/multiprocessing.html)
- [PyCUDA](https://documen.tician.de/pycuda/)
- [PyOpenCL](https://documen.tician.de/pyopencl/)
- [Joblib](https://joblib.readthedocs.io/en/stable/) Parallel only for CPU
- [Ray](https://docs.ray.io/en/latest/)
- [Dask](https://dask.org/)
- [RAPIDS](https://rapids.ai/)

## Distributed & Accelerate Training Methods for LLMs

- [HuggingFace Model Parallel](https://huggingface.co/docs/transformers/v4.15.0/parallelism)
- Data Parallel [PyTorch Distributed: Experiences on Accelerating Data Parallel Training](https://arxiv.org/pdf/2006.15704.pdf)
- Tensor Parallel [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf)
- Pipeline Parallel
- 3D Parallel [DeepSpeed: Extreme-scale model training for everyone](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)
- Mixed Precision Training
- API Solutions:
  - DeepSpeed
  - Megatron-LM
  - PyTorch
  - SageMaker
  - FairScale
